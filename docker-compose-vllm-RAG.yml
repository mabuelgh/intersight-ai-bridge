# mabuelgh - Sep 2025

name: vllm-and-open-webui
services:
  vllm:
    image: vllm/vllm-openai:latest
    container_name: vllm
    command: --model /app/model/ --gpu-memory-utilization=0.4 --max-model-len=2048
    volumes:
      - ./models/Phi-4-mini-instruct:/app/model
    ports:
      - "8000:8000"
    restart: always
    ipc: host
    environment:
      - VLLM_USE_FLASHINFER_SAMPLER=0
    runtime: nvidia
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

  vllm-embedded-llm:
    image: vllm/vllm-openai:latest
    container_name: vllm-embedded-llm
    command: --model /app/model/rag --gpu-memory-utilization=0.4
    volumes:
      - ./models/Qwen3-Embedding-0.6B:/app/model/rag
    ports:
      - "8001:8000"
    restart: always
    ipc: host
    environment:
      - VLLM_USE_FLASHINFER_SAMPLER=0
    runtime: nvidia
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]

  open-webui:
    image: ghcr.io/open-webui/open-webui:main
    container_name: open-webui-for-vllm
    volumes:
    - ./open-webui:/app/backend/data
    ports:
      - "3000:8080"
    environment:
      - OPENAI_API_BASE_URL=http://vllm:8000/v1
      - OFFLINE_MODE=true
      - OPENAI_API_KEY=none
      - WEBUI_AUTH=false
    restart: always